# Validation-Aware Coding Prompt

**Universal implementation prompt with agnostic functional validation**

This prompt guides ralph-orchestrator through implementation tasks with automatic functional validation based on the project type detected by DIAGNOSTIC_PROMPT.md.

## Context

**Configuration Files:**
- @validation_config.json - Project type and validation gates (generated by Session 0)
- @.agent/diagnostic_report.md - Project analysis summary

**Validation Module:**
- @src/ralph_orchestrator/validation/ - Validation gate implementations

## Objective

Implement features while verifying each change through appropriate functional validation:
- **Web projects**: Browser automation (Puppeteer/Playwright MCP)
- **iOS/macOS projects**: Simulator control (xc-mcp)
- **CLI tools**: Shell execution and output validation
- **API servers**: HTTP request validation
- **Libraries**: Import and function call testing

## Execution Protocol

### Phase 1: Load Validation Configuration

Before any implementation, load and understand the validation setup:

```python
# Pseudo-code for understanding validation context
with open("validation_config.json") as f:
    config = json.load(f)

project_type = config["project_type"]
validation_gates = config["validation_gates"]
mcp_servers = config["mcp_servers"]
security = config["security"]

# Log the validation strategy
print(f"Project type: {project_type}")
print(f"Validation gates: {[g['id'] for g in validation_gates]}")
print(f"MCP servers: {list(mcp_servers.keys())}")
```

### Phase 2: Implementation Cycle

For each task or feature:

#### Step 1: UNDERSTAND
Read relevant code, understand the change needed.

#### Step 2: IMPLEMENT
Write the code change.

#### Step 3: BUILD (Compilation Gate)
Execute the build validation gate:

```bash
# From validation_config.json["validation_gates"] where type == "compilation"
{build_command}
```

If build fails:
- Fix the compilation error
- Repeat Step 2-3

#### Step 4: TEST (Unit Test Gate)
Execute the test validation gate:

```bash
# From validation_config.json["validation_gates"] where type == "test_runner"
{test_command}
```

If tests fail:
- Fix the failing tests OR update tests if behavior changed intentionally
- Repeat Step 2-4

#### Step 5: VALIDATE (Functional Gate)
Execute functional validation based on project type:

<validation_types>

**Web Projects (type: "web")**
```
Use the configured MCP server (puppeteer or playwright):

1. Start the development server if not running
2. Navigate to the application URL
3. Execute validation steps from config
4. Take screenshots as evidence
5. Verify expected behavior

MCP Tools to use:
- browser_navigate: Load pages
- browser_snapshot: Capture accessibility tree
- browser_click: Interact with elements
- browser_type: Input text
- browser_take_screenshot: Evidence capture
```

**iOS Projects (type: "ios")**
```
Use xc-mcp server:

1. Build the app if not already built
2. Boot the simulator (from config or default)
3. Install the app bundle
4. Launch the app
5. Execute validation steps
6. Take screenshots as evidence

MCP Tools to use:
- simctl-boot: Start simulator
- simctl-install: Install app
- simctl-launch: Launch app
- screenshot: Capture screen
- idb-ui-tap: Tap interactions
- idb-ui-input: Text input
```

**CLI Projects (type: "cli")**
```
Execute commands directly:

1. Run the CLI command with test inputs
2. Capture stdout/stderr
3. Verify exit codes
4. Check output content
5. Verify any output files created

Validation method:
- Use bash/shell execution
- Compare exit codes
- Pattern match output
- File existence checks
```

**API Projects (type: "api")**
```
Make HTTP requests:

1. Start the server if not running
2. Execute test requests (GET, POST, etc.)
3. Verify response status codes
4. Check response body content
5. Validate headers if needed

Validation method:
- Use curl or HTTP MCP if available
- Compare status codes
- JSON path validation
- Response time checks
```

</validation_types>

If functional validation fails:
- Analyze the failure
- Fix the issue (could be code, could be validation config)
- Repeat Step 2-5

#### Step 6: COMMIT
Once all validations pass:

```bash
git add -A
git commit -m "feat: {description of change}

- Validated with {project_type} functional testing
- All validation gates passed"
```

### Phase 3: Iteration Protocol

Continue implementing features following the cycle above until:
1. All planned tasks are complete
2. All validation gates pass consistently
3. No regressions detected

## Validation Gate Reference

### Reading the Config

```json
{
  "validation_gates": [
    {
      "id": "build",
      "type": "compilation",
      "command": "npm run build",
      "success_criteria": { "exit_code": 0 }
    },
    {
      "id": "functional",
      "type": "web",
      "mcp_server": "playwright",
      "validation_steps": [
        { "action": "navigate", "target": "http://localhost:3000" },
        { "action": "snapshot" },
        { "action": "click", "target": "button[type='submit']" }
      ]
    }
  ]
}
```

### Executing Steps by Type

| Gate Type | How to Execute |
|-----------|---------------|
| compilation | Run `command` in shell, check exit code |
| test_runner | Run `command` in shell, check pass rate |
| web | Use MCP tools from `mcp_server` |
| ios | Use xc-mcp tools |
| cli | Run commands in shell |
| api | Make HTTP requests |

### Success Evaluation

For each validation step, evaluate success based on `expected`:
- `"page_loads"` - Page rendered without errors
- `"has_content"` - Response/screen has meaningful content
- `"has_ui_elements"` - UI elements are visible
- `"booted"` - Simulator is running
- `"installed"` - App installed successfully
- `"running"` - Process/app is running
- `{ "exit_code": 0 }` - Command succeeded
- `{ "contains": "text" }` - Output contains text
- `{ "status": 200 }` - HTTP status matches

## Security Constraints

From `validation_config.json["security"]`:

**Allowed Commands**: Only execute commands in the `allowed_commands` list.

**Restricted Paths**: Never access paths in `restricted_paths`.

```python
# Check before executing any command
def is_allowed(command):
    base_cmd = command.split()[0]
    return base_cmd in config["security"]["allowed_commands"]
```

## MCP Server Usage

### Enabling Servers

```python
# The validation config specifies which MCP servers to enable
for server_name, server_config in config["mcp_servers"].items():
    if server_config["enabled"]:
        # Enable this MCP server
        # Only use tools in server_config["tools"]
        pass
```

### Tool Prefixes

MCP tools are prefixed with the server name:
- `mcp__puppeteer__browser_navigate`
- `mcp__playwright__browser_click`
- `mcp__xc-mcp__simctl-boot`

Use the tools specified in the config's `tools` list for each server.

## Example Validation Flows

### Web Application Flow

```
1. Read validation_config.json → project_type: "web_spa"
2. Implement feature
3. Run: npm run build (compilation gate)
4. Run: npm test (test_runner gate)
5. Use playwright MCP:
   - browser_navigate("http://localhost:3000")
   - browser_snapshot()
   - browser_click("#new-feature-button")
   - browser_take_screenshot()
6. Verify expected behavior in screenshot/snapshot
7. Commit if all passed
```

### iOS Application Flow

```
1. Read validation_config.json → project_type: "ios_app"
2. Implement feature
3. Run: xcodebuild build (compilation gate)
4. Run: xcodebuild test (test_runner gate)
5. Use xc-mcp:
   - simctl-boot("iPhone 15 Pro")
   - simctl-install(udid, "build/MyApp.app")
   - simctl-launch(udid, "com.example.MyApp")
   - screenshot()
6. Verify app state in screenshot
7. Commit if all passed
```

### CLI Tool Flow

```
1. Read validation_config.json → project_type: "cli"
2. Implement feature
3. Run: pip install -e . (compilation gate)
4. Run: pytest tests/ (test_runner gate)
5. Execute CLI commands:
   - mycli --help (expect exit 0, contains "Usage")
   - mycli process input.txt (expect exit 0, creates output.txt)
6. Verify exit codes and output files
7. Commit if all passed
```

## Error Recovery

If validation fails repeatedly:

1. **Build failures**: Focus on syntax/type errors
2. **Test failures**: Check if tests need updating or code has bug
3. **Functional failures**: May indicate UX issue or validation config mismatch
4. **MCP tool errors**: Check if server is running, tool is enabled

For persistent failures:
- Review the diagnostic_report.md for project understanding
- Consider updating validation_config.json if config is wrong
- Ask for human intervention if blocked

## Output

After each implementation session, update:
- `.agent/validation_results.json` - Latest validation results
- Commit message should reference validation status

## Success Criteria

1. All validation gates pass (build, test, functional)
2. No regressions in existing functionality
3. Evidence captured (screenshots, logs)
4. Changes committed with validation context

---

**Status**: Ready for execution
**Requires**: validation_config.json (from DIAGNOSTIC_PROMPT.md)
