# Meta-Ralph: Hat Collection Preset Evaluator
# Purpose: Systematically test each hat collection preset with Kiro CLI
# Output: Consolidated findings in tools/preset-evaluation-findings.md

# Event loop settings - generous limits for thorough evaluation
event_loop:
  prompt_file: "tools/PRESET_EVALUATOR_PROMPT.md"
  completion_promise: "EVALUATION_COMPLETE"
  max_iterations: 200              # Allow many iterations for 12 presets
  max_runtime_seconds: 28800       # 8 hours max
  max_consecutive_failures: 3
  starting_event: "evaluation.start"  # Ralph publishes this after coordination

# Use Kiro CLI as the backend
cli:
  backend: "kiro"
  prompt_mode: "arg"
  pty_mode: false
  pty_interactive: false           # Non-interactive for batch evaluation
  idle_timeout_secs: 120           # 2 min timeout per action

# Core behaviors
core:
  specs_dir: "./specs/"
  guardrails:
    - "Document ALL findings - both successes and failures"
    - "Be specific about error messages and stack traces"
    - "Note timing and performance observations"
    - "Capture UX friction points as you experience them"
    - "Confidence protocol: score decisions 0-100. >80 proceed autonomously; 50-80 proceed + document in .ralph/agent/decisions.md; <50 choose safe default + document."

# Single-hat mode for the meta-evaluator
# Ralph handles task.start for coordination, then delegates via evaluation.start
hats:
  evaluator:
    name: "ðŸ§ª Preset Evaluator"
    description: "Tests and documents Ralph's hat collection presets."
    triggers: ["evaluation.start"]
    publishes: ["evaluation.complete"]
    instructions: |
      You are evaluating Ralph's hat collection presets.

      Your job:
      1. Manually test each preset with a representative task
      2. Document what works and what doesn't
      3. Identify UX improvements and bugs
      4. Save all findings to tools/preset-evaluation-findings.md
